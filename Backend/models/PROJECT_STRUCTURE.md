# ğŸ“ PROJECT STRUCTURE - AFTER CLEANUP

**Last Updated**: 2025-10-25  
**Status**: âœ… Cleaned and Organized

---

## ğŸ¯ **PROJECT OVERVIEW**

This project contains an advanced banking credit risk model with:
- **Source Data**: Loan dataset with 2.26M records
- **Main Model**: LightGBM with advanced feature engineering
- **Optimization**: Multiple experiments with class weights and feature selection
- **Results**: Best model (Exp1) with 0.754 AUC and near-zero overfitting

---

## ğŸ“‚ **DIRECTORY STRUCTURE**

```
Tests/
â”œâ”€â”€ ğŸ“„ README.md                          # Main project README
â”œâ”€â”€ ğŸ“„ ADVANCED_MODEL_SUMMARY.md          # Model performance summary
â”œâ”€â”€ ğŸ“„ FEATURE_MAPPING_TABLE.md           # Important reference
â”œâ”€â”€ ğŸ“„ MODEL_OPTIMIZATION_PLAN.md         # Active optimization plan
â”œâ”€â”€ ğŸ“„ SETUP_DB.md                        # Database setup guide
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md               # This file
â”‚
â”œâ”€â”€ ğŸ“ dataset/
â”‚   â””â”€â”€ loan_reduced.csv                  # â­ SOURCE DATA (287 MB, 2.26M rows)
â”‚
â”œâ”€â”€ ğŸ“ datasetModel/                      # â­ MAIN WORKING DIRECTORY
â”‚   â”œâ”€â”€ ğŸ“„ README.md                      # DatasetModel overview
â”‚   â”œâ”€â”€ ğŸ“„ TRAINING_GUIDE.md              # Quick start guide
â”‚   â”œâ”€â”€ ğŸ“„ IMPLEMENTATION_PLAN.md         # Implementation details
â”‚   â”œâ”€â”€ ğŸ“„ MODEL_OPTIMIZATION_PLAN.md     # Optimization strategy
â”‚   â”œâ”€â”€ ğŸ“„ OPTIMIZATION_RESULTS_REPORT.md # Optimization results
â”‚   â”œâ”€â”€ ğŸ“„ FINAL_IMPROVEMENTS_REPORT.md   # Final improvements summary
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ CORE SCRIPTS:
â”‚   â”‚   â”œâ”€â”€ advanced_banking_model.py     # â­ Main model architecture
â”‚   â”‚   â”œâ”€â”€ loan_feature_engineering.py   # â­ Feature engineering
â”‚   â”‚   â”œâ”€â”€ train_from_loan_dataset.py    # â­ Main training script
â”‚   â”‚   â””â”€â”€ model_optimizer.py            # â­ Optimization experiments
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ models/
â”‚   â”‚   â”œâ”€â”€ advanced_banking_model.txt    # Main trained model
â”‚   â”‚   â”œâ”€â”€ advanced_scaler.pkl           # Feature scaler
â”‚   â”‚   â”œâ”€â”€ advanced_model_metadata.json  # Model metadata
â”‚   â”‚   â”œâ”€â”€ model_Exp1_25features_balanced.txt    # ğŸ† BEST MODEL
â”‚   â”‚   â”œâ”€â”€ scaler_Exp1_25features_balanced.pkl
â”‚   â”‚   â”œâ”€â”€ model_Exp2_25features_real_ratio.txt
â”‚   â”‚   â”œâ”€â”€ scaler_Exp2_25features_real_ratio.pkl
â”‚   â”‚   â”œâ”€â”€ model_Exp3_20features_balanced.txt
â”‚   â”‚   â””â”€â”€ scaler_Exp3_20features_balanced.pkl
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ reports/
â”‚   â”‚   â”œâ”€â”€ advanced_model_performance.json       # â­ Main results
â”‚   â”‚   â”œâ”€â”€ advanced_feature_importance.csv       # â­ Feature ranking
â”‚   â”‚   â”œâ”€â”€ calibration_results.json              # Calibration results
â”‚   â”‚   â”œâ”€â”€ credit_offers.json                    # Credit offers (142 MB)
â”‚   â”‚   â””â”€â”€ optimization_results.json             # â­ Optimization results
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ plots/
â”‚       â”œâ”€â”€ advanced_shap_summary.png             # SHAP explanations
â”‚       â””â”€â”€ advanced_feature_importance.png       # Feature importance plot
â”‚
â”œâ”€â”€ ğŸ“ Core Scripts/                      # Supporting scripts
â”‚   â”œâ”€â”€ etl_customer_data.py             # Nessie API integration
â”‚   â”œâ”€â”€ labels_improved.py               # Active labeling logic
â”‚   â”œâ”€â”€ model_gbm.py                     # Baseline model (if needed)
â”‚   â”œâ”€â”€ db_config.py                     # Database config (if using DB)
â”‚   â”œâ”€â”€ create_database.py               # DB creation (if using DB)
â”‚   â””â”€â”€ populate_data_db.py              # DB population (if using DB)
â”‚
â”œâ”€â”€ ğŸ“ archive/
â”‚   â””â”€â”€ old_docs/                        # Historical documentation
â”‚       â”œâ”€â”€ PHASE3_SUMMARY.md
â”‚       â”œâ”€â”€ PHASE3_IMPROVED_SUMMARY.md
â”‚       â”œâ”€â”€ PHASE5_SUMMARY.md
â”‚       â”œâ”€â”€ MIGRATION_SUMMARY.md
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt                   # Python dependencies
â””â”€â”€ ğŸ“œ quick_test.sh                      # Quick test script
```

---

## ğŸ¯ **KEY FILES EXPLANATION**

### **Main Training Workflow:**

1. **Source Data**: `dataset/loan_reduced.csv` (287 MB, 2.26M rows)
2. **Training**: `datasetModel/train_from_loan_dataset.py`
3. **Feature Engineering**: `datasetModel/loan_feature_engineering.py`
4. **Model Architecture**: `datasetModel/advanced_banking_model.py`
5. **Optimization**: `datasetModel/model_optimizer.py`

### **Best Model:**
- **File**: `datasetModel/models/model_Exp1_25features_balanced.txt`
- **AUC**: 0.754 (+0.030 vs baseline)
- **Overfitting**: 0.004 gap (vs 0.212 baseline)
- **Features**: 24 (optimized from 28)

### **Results:**
- **Performance**: `datasetModel/reports/advanced_model_performance.json`
- **Features**: `datasetModel/reports/advanced_feature_importance.csv`
- **Optimization**: `datasetModel/reports/optimization_results.json`

---

## ğŸ“Š **DISK SPACE**

| Category | Size | Count |
|----------|------|-------|
| Source data | 287 MB | 1 file |
| Models | ~50 MB | 9 files |
| Reports | ~145 MB | 5 files |
| Code | ~1 MB | ~15 files |
| Documentation | ~500 KB | ~12 files |
| **TOTAL** | **~483 MB** | **~42 files** |

**Saved**: ~840 MB from cleanup (deleted intermediate/generated files)

---

## ğŸš€ **HOW TO USE**

### **1. Quick Test:**
```bash
cd datasetModel
bash quick_test.sh
```

### **2. Full Training:**
```bash
cd datasetModel
python train_from_loan_dataset.py
```

### **3. Optimization Experiments:**
```bash
cd datasetModel
python model_optimizer.py
```

### **4. Load Results:**
```python
import json
import pandas as pd

# Load results
with open('datasetModel/reports/advanced_model_performance.json', 'r') as f:
    results = json.load(f)

# Load feature importance
importance = pd.read_csv('datasetModel/reports/advanced_feature_importance.csv')
```

---

## âœ… **CLEANUP SUMMARY**

**Deleted:**
- ~53 files (~840 MB)
- Redundant intermediate datasets
- Old test data
- Obsolete scripts
- Duplicate documentation

**Kept:**
- Source data (`loan_reduced.csv`)
- Core model scripts
- Best trained models
- Current results and reports
- Essential documentation

**Archived:**
- Historical phase summaries
- Old documentation

---

## ğŸ“ **NOTES**

1. **Intermediate datasets** (enhanced/improved) can be regenerated by running `train_from_loan_dataset.py`
2. **Models** are trained and saved automatically during execution
3. **Reports** are generated automatically after training
4. **Archive** folder contains historical documentation for reference
5. **Source data** is the single source of truth (`loan_reduced.csv`)

---

**Last Cleanup**: 2025-10-25  
**Cleanup Script**: `cleanup_project.sh`
