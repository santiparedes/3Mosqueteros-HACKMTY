# Plan de Optimizaci√≥n del Modelo

## üéØ Objetivos

1. **Mejorar balance de clases** - Optimizar SMOTE y class weights para validation/test
2. **Reducir overfitting** - Eliminar features de baja importancia (SHAP < 0.01)
3. **Mejorar generalizaci√≥n** - Reducir gap entre Train (0.935) y Test (0.724)

---

## üìä An√°lisis del Estado Actual

### Problema 1: Desbalance Severo en Validation/Test

| Dataset | n_good | n_bad | % good | % bad |
|---------|--------|-------|--------|-------|
| **Train** | 1,380,677 | 1,380,677 | 50.0% | 50.0% ‚úì |
| **Validation** | 43,340 | 295,760 | 12.8% | 87.2% ‚ö†Ô∏è |
| **Test** | 42,951 | 296,150 | 12.7% | 87.3% ‚ö†Ô∏è |

**Impacto:**
- El modelo aprende con balance 50/50 pero predice en 12/88
- Esto causa que las probabilidades predichas est√©n mal calibradas
- Las m√©tricas de precision/recall est√°n sesgadas

**Soluci√≥n:** Aplicar class weights din√°micos que reflejen el desbalance real

---

### Problema 2: Features de Baja Importancia

| Feature | SHAP Importance | Acci√≥n |
|---------|----------------|--------|
| `income_debt_interaction` | 0.0000 | ‚ùå **ELIMINAR** |
| `spending_monthly` | 0.0016 | ‚ùå **ELIMINAR** |
| `income_adequacy` | 0.0066 | ‚ùå **ELIMINAR** |
| `creditworthiness_score` | 0.0077 | ‚ö†Ô∏è **REVISAR** |
| `dti_health_score` | 0.0127 | ‚ö†Ô∏è **REVISAR** |

**Features a mantener (SHAP > 0.01):**
- `payment_consistency` (1.034) ‚≠ê
- `debt_service_ratio` (0.811) ‚≠ê
- `payroll_variance` (0.746) ‚≠ê
- `income_stability_score` (0.398)
- `spending_var_6m` (0.393)
- ... (resto con importancia > 0.01)

**Impacto:**
- 4-5 features a√±aden ruido sin informaci√≥n √∫til
- Aumentan complejidad y riesgo de overfitting
- Ralentizan inferencia

**Soluci√≥n:** Eliminar features con SHAP < 0.01

---

### Problema 3: Overfitting

**Evidencia:**
- Train AUC: 0.935
- Test AUC: 0.724
- **Gap: 0.211** (21.1% de degradaci√≥n)

**Causas:**
1. Demasiados features (29 features, algunos irrelevantes)
2. SMOTE aplicado solo en train (crea distribuci√≥n artificial)
3. Posible overfit en features de baja importancia

**Soluci√≥n:** Combinar feature selection + class weights + regularizaci√≥n

---

## üõ†Ô∏è Plan de Implementaci√≥n

### **Mejora 1: Optimizaci√≥n de Class Balance**

#### Opci√≥n A: Class Weights Din√°micos (RECOMENDADO)

**Ventajas:**
- No modifica la distribuci√≥n original de datos
- Mejor calibraci√≥n de probabilidades
- M√°s realista para producci√≥n

**Implementaci√≥n:**

```python
# Calcular class weight basado en distribuci√≥n real de validation/test
neg_count = y_train.value_counts()[0]  # good
pos_count = y_train.value_counts()[1]  # bad

# Opci√≥n 1: Balanceo simple
scale_pos_weight = neg_count / pos_count

# Opci√≥n 2: Balanceo suave (menos agresivo)
scale_pos_weight = np.sqrt(neg_count / pos_count)

# Opci√≥n 3: Basado en target real de validation
actual_ratio = len(y_valid[y_valid==0]) / len(y_valid[y_valid==1])
scale_pos_weight = actual_ratio
```

**Estrategia:**
1. **Eliminar SMOTE** completamente
2. **Usar class_weight='balanced'** en LightGBM
3. **Ajustar scale_pos_weight** basado en ratio real de validation/test (~7:1)

#### Opci√≥n B: SMOTE Mejorado

**Solo si Opci√≥n A no funciona:**

```python
# SMOTE con ratio m√°s cercano a realidad
smote = SMOTE(
    sampling_strategy=0.3,  # 30% minority vs 70% majority
    random_state=42,
    k_neighbors=3
)
```

**Comparaci√≥n:**

| M√©todo | Train Balance | Calibraci√≥n | Velocidad | Recomendado |
|--------|--------------|-------------|-----------|-------------|
| **Class Weights** | Natural (12/88) | Excelente | R√°pido | ‚úÖ S√ç |
| **SMOTE 50/50** | Artificial (50/50) | Pobre | Lento | ‚ùå NO |
| **SMOTE 30/70** | Semi-artificial | Buena | Medio | ‚ö†Ô∏è Backup |

---

### **Mejora 2: Feature Selection Autom√°tica**

#### Estrategia de Eliminaci√≥n

**Fase 1: Eliminaci√≥n Agresiva (SHAP < 0.01)**

Features a eliminar:
```python
LOW_IMPORTANCE_FEATURES = [
    'income_debt_interaction',      # 0.0000
    'spending_monthly',              # 0.0016
    'income_adequacy',               # 0.0066
    'creditworthiness_score',        # 0.0077
]
```

**Impacto esperado:**
- Reducci√≥n: 29 ‚Üí 25 features (-13.8%)
- Mejora en generalizaci√≥n
- Reducci√≥n de overfitting

**Fase 2: An√°lisis de Correlaci√≥n**

Eliminar features redundantes (correlaci√≥n > 0.9):

```python
# Detectar pares altamente correlacionados
correlation_matrix = df[features].corr()
high_corr_pairs = []

for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > 0.9:
            feat1 = correlation_matrix.columns[i]
            feat2 = correlation_matrix.columns[j]

            # Mantener el de mayor SHAP
            if importance[feat1] > importance[feat2]:
                remove_features.append(feat2)
            else:
                remove_features.append(feat1)
```

**Candidatos a revisar:**
- `income_volatility` vs `payroll_variance` (probablemente alta correlaci√≥n)
- `dti` vs `debt_service_ratio`
- `utilization` vs `credit_utilization_health`

---

## üìã Implementaci√≥n Step-by-Step

### **Fase 1: An√°lisis y Preparaci√≥n** (D√≠as 1-2)

#### Paso 1.1: An√°lisis de Correlaci√≥n
```bash
python analyze_feature_correlations.py
```

**Output esperado:**
- `reports/feature_correlation_matrix.csv`
- `reports/redundant_features.json`
- `plots/correlation_heatmap.png`

#### Paso 1.2: Validaci√≥n de Features
```bash
python validate_feature_importance.py
```

**Output esperado:**
- Lista de features a eliminar
- Comparaci√≥n SHAP vs correlaci√≥n
- Recomendaciones finales

---

### **Fase 2: Implementaci√≥n de Mejoras** (D√≠as 3-5)

#### Paso 2.1: Crear Script de Optimizaci√≥n

**Archivo:** `optimize_model.py`

**Funcionalidad:**
1. Eliminar features de baja importancia
2. Entrenar con class weights din√°micos
3. Comparar con modelo baseline
4. Generar reporte de mejoras

#### Paso 2.2: Experimentaci√≥n con Class Weights

**Experimento 1:** Sin SMOTE, class_weight='balanced'
```python
params = {
    'scale_pos_weight': None,  # Auto-calculado por 'balanced'
    'class_weight': 'balanced'
}
```

**Experimento 2:** Sin SMOTE, scale_pos_weight manual
```python
# Ratio real de validation/test ‚âà 7:1
params = {
    'scale_pos_weight': 7.0
}
```

**Experimento 3:** SMOTE conservador
```python
smote = SMOTE(sampling_strategy=0.3)  # 30% minority
params = {
    'scale_pos_weight': 1.0
}
```

#### Paso 2.3: Feature Selection Iterativa

**Estrategia:**
1. **Baseline:** Modelo actual (29 features)
2. **Experimento 1:** Eliminar SHAP < 0.01 (25 features)
3. **Experimento 2:** Eliminar SHAP < 0.02 (20 features)
4. **Experimento 3:** Eliminar correlaci√≥n > 0.9 (15-18 features)

**M√©trica de comparaci√≥n:** Test AUC-ROC

---

### **Fase 3: Validaci√≥n y Comparaci√≥n** (D√≠as 6-7)

#### Paso 3.1: Experimentos Combinados

| Experimento | Features | SMOTE | Class Weight | Expected AUC |
|-------------|----------|-------|--------------|--------------|
| **Baseline** | 29 | 50/50 | 1.0 | 0.724 |
| **Exp 1** | 25 | No | balanced | 0.730-0.750 |
| **Exp 2** | 25 | No | 7.0 | 0.730-0.750 |
| **Exp 3** | 20 | No | balanced | 0.735-0.760 |
| **Exp 4** | 20 | 30/70 | 2.3 | 0.730-0.755 |

#### Paso 3.2: M√©tricas de Validaci√≥n

Para cada experimento, medir:

```python
metrics = {
    'auc_roc': ...,
    'pr_auc': ...,
    'brier_score': ...,
    'precision': ...,
    'recall': ...,
    'train_test_gap': ...,  # NUEVO: Train AUC - Test AUC
    'calibration_error': ...,  # NUEVO
    'inference_time': ...,  # NUEVO
}
```

**Criterios de √©xito:**
- ‚úÖ Test AUC ‚â• 0.73 (+0.006)
- ‚úÖ Train-Test gap ‚â§ 0.15 (-0.06)
- ‚úÖ Brier score ‚â§ 0.12 (-0.006)
- ‚úÖ Reducci√≥n de features ‚â• 10%

---

## üöÄ Scripts a Crear

### 1. `analyze_feature_correlations.py`

**Prop√≥sito:** Identificar features redundantes

**Output:**
- Matriz de correlaci√≥n
- Lista de pares correlacionados (> 0.9)
- Recomendaciones de eliminaci√≥n

---

### 2. `optimize_class_weights.py`

**Prop√≥sito:** Encontrar optimal class weight

**M√©todo:**
```python
# Grid search sobre scale_pos_weight
weights_to_test = [1.0, 3.0, 5.0, 7.0, 10.0, 15.0]

for weight in weights_to_test:
    model = train_with_weight(weight)
    auc = evaluate(model, X_valid, y_valid)
    results[weight] = auc

optimal_weight = max(results, key=results.get)
```

**Output:**
- Gr√°fico de AUC vs class_weight
- Optimal weight value
- Comparaci√≥n con SMOTE

---

### 3. `feature_selection_pipeline.py`

**Prop√≥sito:** Selecci√≥n autom√°tica de features

**M√©todo:**
```python
# Recursive Feature Elimination con SHAP
features_ranked = shap_importance.sort_values(ascending=False)
feature_sets = []

# Probar con diferentes thresholds
for threshold in [0.001, 0.005, 0.01, 0.02, 0.05]:
    selected = features_ranked[features_ranked > threshold]
    feature_sets.append(selected.index.tolist())

# Entrenar modelo con cada feature set
for features in feature_sets:
    model = train_with_features(features)
    results[len(features)] = evaluate(model)
```

**Output:**
- Curva de AUC vs n√∫mero de features
- Optimal feature set
- Feature importance final

---

### 4. `model_optimizer.py` (SCRIPT PRINCIPAL)

**Prop√≥sito:** Pipeline completo de optimizaci√≥n

**Workflow:**
```python
# 1. Cargar datos
train, valid, test = load_data()

# 2. Analizar correlaciones
redundant = find_redundant_features(train)

# 3. Eliminar low SHAP
low_importance = [f for f in features if shap[f] < 0.01]
features = [f for f in features if f not in low_importance]

# 4. Optimizar class weights
optimal_weight = find_optimal_weight(train, valid)

# 5. Entrenar modelo optimizado
model = train_optimized_model(
    features=features,
    scale_pos_weight=optimal_weight,
    use_smote=False
)

# 6. Evaluar y comparar
compare_with_baseline(model, baseline_model)

# 7. Guardar resultados
save_optimization_report()
```

---

### 5. `compare_models.py`

**Prop√≥sito:** Comparaci√≥n lado a lado

**Output:**
```
================================================================================
MODEL COMPARISON REPORT
================================================================================

BASELINE MODEL:
  Features: 29
  SMOTE: Yes (50/50)
  Class Weight: 1.0
  Test AUC: 0.7236
  Train-Test Gap: 0.2117

OPTIMIZED MODEL:
  Features: 22
  SMOTE: No
  Class Weight: 7.0 (balanced)
  Test AUC: 0.7458 (+0.0222)
  Train-Test Gap: 0.1534 (-0.0583)

IMPROVEMENTS:
  ‚úÖ AUC improved by +2.22%
  ‚úÖ Overfitting reduced by -27.5%
  ‚úÖ Features reduced by -24.1%
  ‚úÖ Inference time: -18%
```

---

## üìä Expected Results

### Mejora Esperada

| M√©trica | Baseline | Target | Mejora |
|---------|----------|--------|--------|
| **Test AUC** | 0.7236 | 0.7450+ | +2.9% |
| **Train-Test Gap** | 0.2117 | 0.1500 | -29.1% |
| **Brier Score** | 0.1260 | 0.1150 | -8.7% |
| **Features** | 29 | 20-22 | -24-31% |
| **Training Time** | 100% | 80% | -20% |

### Riesgos y Mitigaci√≥n

| Riesgo | Probabilidad | Mitigaci√≥n |
|--------|-------------|------------|
| AUC no mejora | Media | Probar m√∫ltiples configuraciones |
| Overfitting persiste | Baja | Aumentar regularizaci√≥n (L1/L2) |
| Features importantes eliminadas | Baja | Usar threshold conservador (0.01) |
| Class weights sub√≥ptimos | Media | Grid search exhaustivo |

---

## ‚è±Ô∏è Timeline

| Fase | Duraci√≥n | Entregables |
|------|----------|-------------|
| **1. An√°lisis** | 1-2 d√≠as | Scripts de an√°lisis, reportes |
| **2. Implementaci√≥n** | 2-3 d√≠as | Scripts de optimizaci√≥n |
| **3. Experimentaci√≥n** | 2-3 d√≠as | Resultados de experimentos |
| **4. Validaci√≥n** | 1 d√≠a | Reporte final, modelo optimizado |
| **Total** | **6-9 d√≠as** | Modelo mejorado en producci√≥n |

---

## üéØ Success Criteria

‚úÖ **Criterios M√≠nimos:**
1. Test AUC ‚â• 0.730 (+0.6%)
2. Train-Test gap ‚â§ 0.180 (-15%)
3. Features reducidos en ‚â• 10%

‚úÖ **Criterios √ìptimos:**
1. Test AUC ‚â• 0.745 (+3%)
2. Train-Test gap ‚â§ 0.150 (-30%)
3. Features reducidos en ‚â• 20%
4. Brier score ‚â§ 0.115 (-9%)

---

## üîÑ Iteraci√≥n Continua

Despu√©s de implementaci√≥n inicial:

**Monitoreo:**
- Track AUC en producci√≥n
- Detectar feature drift
- Alertas si AUC < threshold

**Re-entrenamiento:**
- Mensual: Re-train con nuevos datos
- Trimestral: Re-evaluar feature importance
- Anual: Re-dise√±ar features si necesario

---

## üìù Next Steps

1. **Hoy:** Revisar y aprobar plan
2. **D√≠a 1:** Crear scripts de an√°lisis
3. **D√≠a 2:** Ejecutar an√°lisis, generar reportes
4. **D√≠a 3-5:** Implementar optimizaciones
5. **D√≠a 6-7:** Validar y comparar resultados
6. **D√≠a 8:** Deploy modelo optimizado

---

**¬øListo para empezar? Puedo crear los scripts de optimizaci√≥n ahora mismo.**
